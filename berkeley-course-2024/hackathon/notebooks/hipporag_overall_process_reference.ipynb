{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any, Dict, List, Literal, Optional\n",
    "\n",
    "from llm_foundation import logger\n",
    "from llm_foundation.agent_types import Persona, Role\n",
    "\n",
    "from crewai import Agent, Task, Crew\n",
    "from crewai.crews import CrewOutput\n",
    "from hackathon.index import generate_embeddings, create_index, search_index, build_similar_entities\n",
    "from hackathon.input_output_types import NamedEntities\n",
    "from hackathon.retrieval_neo4j import chunk_ranker, retrieve_context, retrieve_similar_entities, pagerank\n",
    "from hackathon.graph_neo4j import add_entities, add_relates_to_relationships, build_vector_index, add_similar_entities, clean_db, Neo4jClientFactory\n",
    "from hackathon.tools import filter_named_entities, create_document_deduped_entities_dict, create_matrix_entity_ref_count\n",
    "from hackathon.utils import build_document_structure, save_document_structure\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    " \n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from rich import print\n",
    "from rich.pretty import pprint\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "CHUNK_LIMIT = 15  # 10 chunks for testing. -1 for all chunks\n",
    "CHAR_OVERLAP = 200\n",
    "document_name = \"../2405.14831v1.pdf\"\n",
    "llm = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Structure of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chunk limit is set to 10 for testing purposes. Set it to -1 to process all chunks.\n",
    "document_chunks = build_document_structure(document_name, chunk_size=CHUNK_SIZE, char_overlap=CHAR_OVERLAP, chunk_limit=CHUNK_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document consist of a List of Chunks\n",
    "Each Chunk is initially a dictionary with the following elements\n",
    "\n",
    "```python\n",
    "{\n",
    "    id: int,\n",
    "    text: str\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract name entities from each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_entities_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Your task is to extract entities from the given paragraph, in the same language as the paragraph.\n",
    "Respond with a JSON list of entities.\"\"\"),\n",
    "        (\"human\", \"\"\"Paragraph:\n",
    "```\n",
    "Radio City\n",
    "Radio City is India's first private FM radio station and was started on 3 July 2001.\n",
    "It plays Hindi, English and regional songs.\n",
    "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n",
    "```\"\"\"),\n",
    "        (\"ai\", \"\"\"{{\"entities\":\n",
    "    [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\", \"English\", \"May 2008\", \"PlanetRadiocity.com\"]\n",
    "}}\"\"\"),\n",
    "        (\"human\", \"\"\"Paragraph:```\n",
    "{passage_text}\n",
    "```\"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "extract_triplets_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Your task is to construct an RDF (Resource Description Framework) graph from the given passages and entity lists. \n",
    "Respond with a JSON list of triples, with each triple representing a relationship in the RDF graph. \n",
    "\n",
    "Pay attention to the following requirements:\n",
    "- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\n",
    "- Clearly resolve pronouns to their specific names to maintain clarity.\n",
    "\"\"\"),\n",
    "        (\"human\", \"\"\"Convert the paragraph into a JSON dict, it has a named entity list and a triple list.\n",
    "Paragraph:\n",
    "```\n",
    "Radio City\n",
    "Radio City is India's first private FM radio station and was started on 3 July 2001.\n",
    "It plays Hindi, English and regional songs.\n",
    "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n",
    "```\n",
    "\n",
    "{{\"entities\":\n",
    "    [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\", \"English\", \"May 2008\", \"PlanetRadiocity.com\"]\n",
    "}}\"\"\"),\n",
    "        (\"ai\", \"\"\"{{\"triples\": [\n",
    "            [\"Radio City\", \"located in\", \"India\"],\n",
    "            [\"Radio City\", \"is\", \"private FM radio station\"],\n",
    "            [\"Radio City\", \"started on\", \"3 July 2001\"],\n",
    "            [\"Radio City\", \"plays songs in\", \"Hindi\"],\n",
    "            [\"Radio City\", \"plays songs in\", \"English\"]\n",
    "            [\"Radio City\", \"forayed into\", \"New Media\"],\n",
    "            [\"Radio City\", \"launched\", \"PlanetRadiocity.com\"],\n",
    "            [\"PlanetRadiocity.com\", \"launched in\", \"May 2008\"],\n",
    "            [\"PlanetRadiocity.com\", \"is\", \"music portal\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"news\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"videos\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"songs\"]\n",
    "    ]\n",
    "}}\"\"\"),\n",
    "        (\"human\", \"\"\"Convert the paragraph into a JSON dict, it has a entity list and a triple list.\n",
    "Paragraph:\n",
    "```\n",
    "{passage_text}\n",
    "```\n",
    "\n",
    "{entities}\"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_document_chunks_with_entities_and_triples(llm_model, document_chunks: List[Dict]) -> List[Dict]:\n",
    "\n",
    "    for chunk in document_chunks:\n",
    "        chunk[\"named_entities\"] =[]\n",
    "        chunk[\"triples\"] = []\n",
    "        try:\n",
    "            json_output_parser = SimpleJsonOutputParser()\n",
    "            chain_entities = extract_entities_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "            named_entities = chain_entities.invoke({\"passage_text\": chunk[\"text\"]})\n",
    "            chunk[\"named_entities\"] = named_entities[\"entities\"]\n",
    "\n",
    "            chain_triples = extract_triplets_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "            triples = chain_triples.invoke({\"passage_text\": chunk[\"text\"], \"entities\": named_entities})\n",
    "            chunk[\"triples\"] = triples[\"triples\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing passage: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks_with_entities_and_triples = extend_document_chunks_with_entities_and_triples(llm, document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each Chunk is has also in the dictionary named entities and triples\n",
    "\n",
    "```python\n",
    "{\n",
    "    id: int,\n",
    "    text: str,\n",
    "    named_entities: List[str]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks_with_entities_and_triples[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_structure_file = f\"{document_name.rsplit(\".\", 1)[0]}_document_structure.pkl\"\n",
    "save_document_structure(document_chunks_with_entities_and_triples, document_structure_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks_with_entities_and_triples = filter_named_entities(document_chunks_with_entities_and_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Document Structure after Filter Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_structure_file_with_ne = f\"{document_name.rsplit(\".\", 1)[0]}_document_structure_with_ne.pkl\"\n",
    "save_document_structure(document_chunks_with_entities_and_triples, document_structure_file_with_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks_with_entities_and_triples[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dedup Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2uid_dict = create_document_deduped_entities_dict(document_chunks_with_entities_and_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2uid_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the entity to uid dict to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{document_name.rsplit(\".\", 1)[0]}_entity2uid_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity2uid_dict, f)\n",
    "logger.info(f\"entity2uid_dict has been saved to {document_name.rsplit(\".\", 1)[0]}_entity2uid_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Creation\n",
    "\n",
    "This matrix is important for pagerank below. Each row is an entity id and each column represents a chunk. The contents of each cell is the number of references to the entity in that paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"manipulatable, likely higher-level, features, which are then routed through the parahippocampal\n",
    "regions (phr) to be indexed by the hippocampus. when they reach the hippocampus , salient signals\n",
    "are included in the hippocampal index and associated with each other.\"\"\"\n",
    "\n",
    "string.count(\"parahippocampal\\nregions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_matrix_entity_ref_count(document_chunks_with_entities_and_triples, entity2uid_dict)\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "pprint(matrix.shape)\n",
    "pprint(matrix)\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detail the name of the entity that is referenced and its count in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_chunk = document_chunks_with_entities_and_triples[0]\n",
    "\n",
    "n_of_entities = len(entity2uid_dict)\n",
    "n_of_chunks = len(document_chunks_with_entities_and_triples)\n",
    "\n",
    "for e_idx in range(n_of_entities):\n",
    "    entity_name = list(entity2uid_dict.keys())[list(entity2uid_dict.values()).index(e_idx)]\n",
    "    logger.info(f\"Entity: {e_idx} {entity_name} Chunk count: {matrix[e_idx][:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the matrix to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{document_name.rsplit(\".\", 1)[0]}_entity_per_chunk_count_matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(matrix, f)\n",
    "logger.info(f\"Entity per chunk count matrix has been saved to {document_name.rsplit(\".\", 1)[0]}_entity_per_chunk_count_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pickle.loads(open(f\"{document_name.rsplit(\".\", 1)[0]}_entity_per_chunk_count_matrix.pkl\", \"rb\").read())\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings and FAISS index params\n",
    "emb_dimension = 256\n",
    "recall_at_k = 3  # how far in the indices/distances we go\n",
    "\n",
    "# M_max defines the maximum number of links a vertex can have, and M_max0, which defines the same but for vertices in layer 0.\n",
    "M = 64  # for HNSW index, the number of neighbors we add to each vertex on insertion. \n",
    "# Faiss sets M_max and M_max0 automatically in the set_default_probas method, at index initialization. \n",
    "# The M_max value is set to M, and M_max0 set to M*2\n",
    "\n",
    "emb_model = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings for named entities in document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities_dict = pickle.loads(open(f\"{document_name.rsplit(\".\", 1)[0]}_entity2uid_dict.pkl\", \"rb\").read())\n",
    "logger.info(f\"Named entities dict loaded: {named_entities_dict}\")\n",
    "entities = list(named_entities_dict.keys())\n",
    "logger.info(f\"Number of entities: {len(entities)}. First entity is: {entities[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate (and save) entity embeddings and convert them to np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filepath = f\"{document_name.rsplit('.', 1)[0]}_entity_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entities_embeddings = generate_embeddings(entities, emb_model, emb_dimension, embeddings_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Step: Load the entity Embeddings (Just to continue from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_embeddings = pickle.loads(open(embeddings_filepath, \"rb\").read())\n",
    "entities_embeddings = np.array(entities_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index and Query It with the same elements we indexed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = create_index(entities_embeddings, emb_dimension, M)\n",
    "distances, indexes = search_index(faiss_index, entities_embeddings, recall_at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Similar Entities with Recall at K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_entities = build_similar_entities(entities, indexes, distances, recall_at_k, max_distance=0.85)  # Original max_distance=0.7\n",
    "logger.info(f\"Similar entities:\\n{similar_entities}\")\n",
    "\n",
    "# TODO Scores discarded for now\n",
    "# scores = calculate_scores(distances)\n",
    "# similar_entities_score = build_similar_entities_with_scores(entities, indexes, scores, recall_at_k, min_score=0.5)            \n",
    "# logger.info(similar_entities_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neo4J Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_factory = Neo4jClientFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required to create the graph:\n",
    "- entity_embeddings\n",
    "- named entities dict \n",
    "- doc structure\n",
    "- similar entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Add all entities to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_db(neo4j_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(named_entities_dict), len(entities_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entities(neo4j_factory, entities_embeddings, named_entities_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add RELATES_TO relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_structure = pickle.loads(open(f\"{document_name.rsplit(\".\", 1)[0]}_document_structure_with_ne.pkl\", \"rb\").read())\n",
    "doc_structure[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_relates_to_relationships(neo4j_factory, doc_structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add SIMILAR_TO relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_similar_entities(neo4j_factory, similar_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_vector_index(neo4j_factory, emb_dim=emb_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_master = Persona.from_yaml_file(\"../Personas/EntityMasterCrewAI.yaml\")\n",
    "# pprint(entity_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Entity Extractor Agent\n",
    "\n",
    "Extracts the entities from the user query to retrieve the best chunks later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What does the hippocampal memory indexing theory propose?\"\n",
    "user_query = \"What are the main three brain regions involved in the hippocampal memory indexing theory?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extractor_role: Role = entity_master.get_role(\"entity_extractor\")\n",
    "pprint(entity_extractor_role)\n",
    "entity_extractor: Agent = entity_extractor_role.to_crewai_agent(verbose=True, allow_delegation=False)\n",
    "\n",
    "extract_entities = Task(\n",
    "    description=entity_extractor_role.tasks[0].description,\n",
    "    expected_output=entity_extractor_role.tasks[0].expected_output,\n",
    "    agent=entity_extractor,\n",
    "    output_json=NamedEntities,\n",
    ")\n",
    "\n",
    "query_inputs = {\n",
    "    \"paragraph\": user_query,\n",
    "    # \"paragraph\": \"What is HippoRAG?\",\n",
    "    \"entity_extractor_examples\": entity_extractor_role.get_examples_as_str(),\n",
    "}\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[entity_extractor],\n",
    "    tasks=[extract_entities],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "logger.info(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Calling Agents ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "result: CrewOutput = crew.kickoff(inputs=query_inputs)\n",
    "\n",
    "logger.info(\".................................................................................\")\n",
    "logger.info(type(result.json))\n",
    "entities = json.loads(result.json)\n",
    "query_entities = entities[\"named_entities\"]\n",
    "logger.info(f\"Query entities: {query_entities}\")\n",
    "logger.info(\".................................................................................\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_nodes = retrieve_similar_entities(neo4j_factory, query_entities)\n",
    "\n",
    "for node in related_nodes:\n",
    "    logger.info(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_score = pagerank(neo4j_factory, related_nodes, matrix)\n",
    "\n",
    "for score in nodes_score:\n",
    "    logger.info(f\"score= {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank the Chunks after PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape, len(nodes_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_score, chunks_order = chunk_ranker(matrix, nodes_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of chunks to see the difference in the output!!!!!\n",
    "\n",
    "# Try with 1, 2, 3...\n",
    "\n",
    "context = retrieve_context(doc_structure, chunks_score, chunks_order, max_chunks=3)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HippoRAG Enhanced Query Agent\n",
    "\n",
    "Answer the question based on the retrieved context using the HippoRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hippo_savant_role: Role = entity_master.get_role(\"hippo_savant\")\n",
    "hippo_savant: Agent = hippo_savant_role.to_crewai_agent(verbose=True, allow_delegation=False)\n",
    "\n",
    "answer_question = Task(\n",
    "    description=hippo_savant_role.tasks[0].description,\n",
    "    expected_output=hippo_savant_role.tasks[0].expected_output,\n",
    "    agent=hippo_savant,\n",
    ")\n",
    "\n",
    "query_inputs = {\n",
    "    \"context\": context,\n",
    "    \"query\": user_query,\n",
    "}\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[hippo_savant],\n",
    "    tasks=[answer_question],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "logger.info(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Calling Agents ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "result: CrewOutput = crew.kickoff(inputs=query_inputs)\n",
    "logger.info(\".................................................................................\")\n",
    "logger.info(f\"Answer:\\n{pprint(result.raw)}\")\n",
    "logger.info(\".................................................................................\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
