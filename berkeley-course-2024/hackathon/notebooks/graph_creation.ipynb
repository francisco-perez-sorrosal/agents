{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:02:41 R2GWRJJGF9 root[36374] INFO Logger root configured\n",
      "2024-11-05 13:02:41 R2GWRJJGF9 faiss.loader[36374] INFO Loading faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to configure logger root in module llm_foundation\n",
      "root # of associated handlers - 0\n",
      "Logging is not configured yet. Configuring it now.\n",
      "Basic logging config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:02:41 R2GWRJJGF9 faiss.loader[36374] INFO Successfully loaded faiss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, List, Literal, Optional\n",
    "\n",
    "from llm_foundation import logger\n",
    "from rich import print\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from hackathon.index import generate_entity_embeddings, create_index, search_index, calculate_scores, build_similar_entities\n",
    "from hackathon.graph_neo4j import add_entities, add_relates_to_relationships, build_vector_index, add_similar_entities, clean_db\n",
    "from hackathon.tools import filter_named_entities, create_document_deduped_entities_dict, create_matrix_entity_ref_count\n",
    "from hackathon.utils import build_document_structure, save_document_structure\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 5000\n",
    "document_name = \"../2405.14831v1.pdf\"\n",
    "llm = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Structure of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:29:10 R2GWRJJGF9 root[40146] INFO --------------------------------------------------------------------------------\n",
      "2024-11-05 11:29:10 R2GWRJJGF9 root[40146] INFO Number of chunks: 19\n",
      "2024-11-05 11:29:10 R2GWRJJGF9 root[40146] INFO --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "document_chunks = build_document_structure(document_name, chunk_size=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document consist of a List of Chunks\n",
    "Each Chunk is initially a dictionary with the following elements\n",
    "\n",
    "```python\n",
    "{\n",
    "    id: int,\n",
    "    text: str\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': 'HippoRAG: Neurobiologically Inspired\\nLong-Term Memory for Large Language Models\\nBernal Jiménez Gutiérrez\\nThe Ohio State University\\njimenezgutierrez.1@osu.eduYiheng Shu\\nThe Ohio State University\\nshu.251@osu.edu\\nYu Gu\\nThe Ohio State University\\ngu.826@osu.eduMichihiro Yasunaga\\nStanford University\\nmyasu@cs.stanford.eduYu Su\\nThe Ohio State University\\nsu.809@osu.edu\\nAbstract\\nIn order to thrive in hostile and ever-changing natural environments, mammalian\\nbrains evolved to store large amounts of knowledge about the world and continually\\nintegrate new information while avoiding catastrophic forgetting. Despite the\\nimpressive accomplishments, large language models (LLMs), even with retrieval-\\naugmented generation (RAG), still struggle to efficiently and effectively integrate\\na large amount of new experiences after pre-training. In this work, we introduce\\nHippoRAG, a novel retrieval framework inspired by the hippocampal indexing\\ntheory of human long-term memory to enable deeper and more efficient knowledge\\nintegration over new experiences. HippoRAG synergistically orchestrates LLMs,\\nknowledge graphs, and the Personalized PageRank algorithm to mimic the different\\nroles of neocortex and hippocampus in human memory. We compare HippoRAG\\nwith existing RAG methods on multi-hop question answering and show that our\\nmethod outperforms the state-of-the-art methods remarkably, by up to 20%. Single-\\nstep retrieval with HippoRAG achieves comparable or better performance than\\niterative retrieval like IRCoT while being 10-30times cheaper and 6-13times faster,\\nand integrating HippoRAG into IRCoT brings further substantial gains. Finally,\\nwe show that our method can tackle new types of scenarios that are out of reach of\\nexisting methods.1\\n1 Introduction\\nMillions of years of evolution have led mammalian brains to develop the crucial ability to store large\\namounts of world knowledge and continuously integrate new experiences without losing previous\\nones. This exceptional long-term memory system eventually allows us humans to keep vast stores of\\ncontinuously updating knowledge that forms the basis of our reasoning and decision making [15].\\nDespite the progress of large language models (LLMs) in recent years, such a continuously updating\\nlong-term memory is still conspicuously absent from current AI systems. Due in part to its ease of\\nuse and the limitations of other techniques such as model editing [ 35], retrieval-augmented generation\\n(RAG) has become the de facto solution for long-term memory in LLMs, allowing users to present\\nnew knowledge to a static model [28, 33, 50].\\nHowever, current RAG methods are still unable to help LLMs perform tasks that require integrating\\nnew knowledge across passage boundaries since each new passage is encoded in isolation. Many\\n1Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG .\\nPreprint. Under review.arXiv:2405.14831v1  [cs.CL]  23 May 2024\\nHumanMemory Current RAG\\nHippoRAG Offline IndexingOnline Retrieval\\nAnswer:\\n!\\n!\\n!\\n!\\n\"\\n#\\n$\\n%\\n!\\n!Which     Stanford professor works on the neuroscience of     Alzheimer’s?\\n!\\n&\\n$\\n%\\n!\\n\"\\n\\'\\n&\\n#\\n&\\n%\\n!\\n\\'\\n#\\n\"\\nProf. ThomasFigure 1: Knowledge Integration & RAG. Tasks that require knowledge integration are particularly\\nchallenging for current RAG systems. In the above example, we want to find a Stanford professor\\nthat does Alzheimer’s research from a pool of passages describing potentially thousands Stanford\\nprofessors and Alzheimer’s researchers. Since current methods encode passages in isolation, they\\nwould struggle to identify Prof. Thomas unless a passage mentions both characteristics at once. In\\ncontrast, most people familiar with this professor would remember him quickly due to our brain’s\\nassociative memory capabilities, thought to be driven by the index structure depicted in the C-shaped\\nhippocampus above (in blue). Inspired by this mechanism, HippoRAG allows LLMs to build and\\nleverage a similar graph of associations to tackle knowledge integration tasks.\\nimportant real-world tasks, such as scientific literature review, legal case briefing, and medical\\ndiagnosis, require knowledge integration across passages or documents. Although less complex,\\nstandard multi-hop question answering (QA) also requires integrating information between passages\\nin a retrieval corpus. In order to solve such tasks, current RAG systems resort to using multiple\\nretrieval and LLM generation steps iteratively to join disparate passages [ 49,61]. Nevertheless, even\\nperfectly executed multi-step RAG is still oftentimes insufficient to accomplish many scenarios of\\nknowledge integration, as we illustrate in what we call path-finding multi-hop questions in Figure 1.\\nIn contrast, our brains are capable of solving challenging knowledge integration tasks like these with\\nrelative ease. The hippocampal memory indexing theory [ 58], a well-established theory of human\\nlong-term memory, offers one plausible explanation for this remarkable ability. Teyler and Discenna'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract name entities from each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_entities_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Your task is to extract entities from the given paragraph, in the same language as the paragraph.\n",
    "Respond with a JSON list of entities.\"\"\"),\n",
    "        (\"human\", \"\"\"Paragraph:\n",
    "```\n",
    "Radio City\n",
    "Radio City is India's first private FM radio station and was started on 3 July 2001.\n",
    "It plays Hindi, English and regional songs.\n",
    "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n",
    "```\"\"\"),\n",
    "        (\"ai\", \"\"\"{{\"entities\":\n",
    "    [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\", \"English\", \"May 2008\", \"PlanetRadiocity.com\"]\n",
    "}}\"\"\"),\n",
    "        (\"human\", \"\"\"Paragraph:```\n",
    "{passage_text}\n",
    "```\"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "extract_triplets_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Your task is to construct an RDF (Resource Description Framework) graph from the given passages and entity lists. \n",
    "Respond with a JSON list of triples, with each triple representing a relationship in the RDF graph. \n",
    "\n",
    "Pay attention to the following requirements:\n",
    "- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\n",
    "- Clearly resolve pronouns to their specific names to maintain clarity.\n",
    "\"\"\"),\n",
    "        (\"human\", \"\"\"Convert the paragraph into a JSON dict, it has a named entity list and a triple list.\n",
    "Paragraph:\n",
    "```\n",
    "Radio City\n",
    "Radio City is India's first private FM radio station and was started on 3 July 2001.\n",
    "It plays Hindi, English and regional songs.\n",
    "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n",
    "```\n",
    "\n",
    "{{\"entities\":\n",
    "    [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\", \"English\", \"May 2008\", \"PlanetRadiocity.com\"]\n",
    "}}\"\"\"),\n",
    "        (\"ai\", \"\"\"{{\"triples\": [\n",
    "            [\"Radio City\", \"located in\", \"India\"],\n",
    "            [\"Radio City\", \"is\", \"private FM radio station\"],\n",
    "            [\"Radio City\", \"started on\", \"3 July 2001\"],\n",
    "            [\"Radio City\", \"plays songs in\", \"Hindi\"],\n",
    "            [\"Radio City\", \"plays songs in\", \"English\"]\n",
    "            [\"Radio City\", \"forayed into\", \"New Media\"],\n",
    "            [\"Radio City\", \"launched\", \"PlanetRadiocity.com\"],\n",
    "            [\"PlanetRadiocity.com\", \"launched in\", \"May 2008\"],\n",
    "            [\"PlanetRadiocity.com\", \"is\", \"music portal\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"news\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"videos\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"songs\"]\n",
    "    ]\n",
    "}}\"\"\"),\n",
    "        (\"human\", \"\"\"Convert the paragraph into a JSON dict, it has a entity list and a triple list.\n",
    "Paragraph:\n",
    "```\n",
    "{passage_text}\n",
    "```\n",
    "\n",
    "{entities}\"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_document_chunks_with_entities_and_triples(llm_model, document_chunks: List[Dict]) -> List[Dict]:\n",
    "\n",
    "    for chunk in document_chunks:\n",
    "        chunk[\"named_entities\"] =[]\n",
    "        chunk[\"triples\"] = []\n",
    "        try:\n",
    "            json_output_parser = SimpleJsonOutputParser()\n",
    "            chain_entities = extract_entities_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "            named_entities = chain_entities.invoke({\"passage_text\": chunk[\"text\"]})\n",
    "            chunk[\"named_entities\"] = named_entities[\"entities\"]\n",
    "\n",
    "            chain_triples = extract_triplets_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "            triples = chain_triples.invoke({\"passage_text\": chunk[\"text\"], \"entities\": named_entities})\n",
    "            chunk[\"triples\"] = triples[\"triples\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing passage: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:33:05 R2GWRJJGF9 httpx[40146] INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-05 11:33:08 R2GWRJJGF9 httpx[40146] INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "document_chunks_with_entities_and_triples = extend_document_chunks_with_entities_and_triples(llm, document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each Chunk is has also in the dictionary named entities and triples\n",
    "\n",
    "```python\n",
    "{\n",
    "    id: int,\n",
    "    text: str,\n",
    "    named_entities: List[str]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': 'HippoRAG: Neurobiologically Inspired\\nLong-Term Memory for Large Language Models\\nBernal Jiménez Gutiérrez\\nThe Ohio State University\\njimenezgutierrez.1@osu.eduYiheng Shu\\nThe Ohio State University\\nshu.251@osu.edu\\nYu Gu\\nThe Ohio State University\\ngu.826@osu.eduMichihiro Yasunaga\\nStanford University\\nmyasu@cs.stanford.eduYu Su\\nThe Ohio State University\\nsu.809@osu.edu\\nAbstract\\nIn order to thrive in hostile and ever-changing natural environments, mammalian\\nbrains evolved to store large amounts of knowledge about the world and continually\\nintegrate new information while avoiding catastrophic forgetting. Despite the\\nimpressive accomplishments, large language models (LLMs), even with retrieval-\\naugmented generation (RAG), still struggle to efficiently and effectively integrate\\na large amount of new experiences after pre-training. In this work, we introduce\\nHippoRAG, a novel retrieval framework inspired by the hippocampal indexing\\ntheory of human long-term memory to enable deeper and more efficient knowledge\\nintegration over new experiences. HippoRAG synergistically orchestrates LLMs,\\nknowledge graphs, and the Personalized PageRank algorithm to mimic the different\\nroles of neocortex and hippocampus in human memory. We compare HippoRAG\\nwith existing RAG methods on multi-hop question answering and show that our\\nmethod outperforms the state-of-the-art methods remarkably, by up to 20%. Single-\\nstep retrieval with HippoRAG achieves comparable or better performance than\\niterative retrieval like IRCoT while being 10-30times cheaper and 6-13times faster,\\nand integrating HippoRAG into IRCoT brings further substantial gains. Finally,\\nwe show that our method can tackle new types of scenarios that are out of reach of\\nexisting methods.1\\n1 Introduction\\nMillions of years of evolution have led mammalian brains to develop the crucial ability to store large\\namounts of world knowledge and continuously integrate new experiences without losing previous\\nones. This exceptional long-term memory system eventually allows us humans to keep vast stores of\\ncontinuously updating knowledge that forms the basis of our reasoning and decision making [15].\\nDespite the progress of large language models (LLMs) in recent years, such a continuously updating\\nlong-term memory is still conspicuously absent from current AI systems. Due in part to its ease of\\nuse and the limitations of other techniques such as model editing [ 35], retrieval-augmented generation\\n(RAG) has become the de facto solution for long-term memory in LLMs, allowing users to present\\nnew knowledge to a static model [28, 33, 50].\\nHowever, current RAG methods are still unable to help LLMs perform tasks that require integrating\\nnew knowledge across passage boundaries since each new passage is encoded in isolation. Many\\n1Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG .\\nPreprint. Under review.arXiv:2405.14831v1  [cs.CL]  23 May 2024\\nHumanMemory Current RAG\\nHippoRAG Offline IndexingOnline Retrieval\\nAnswer:\\n!\\n!\\n!\\n!\\n\"\\n#\\n$\\n%\\n!\\n!Which     Stanford professor works on the neuroscience of     Alzheimer’s?\\n!\\n&\\n$\\n%\\n!\\n\"\\n\\'\\n&\\n#\\n&\\n%\\n!\\n\\'\\n#\\n\"\\nProf. ThomasFigure 1: Knowledge Integration & RAG. Tasks that require knowledge integration are particularly\\nchallenging for current RAG systems. In the above example, we want to find a Stanford professor\\nthat does Alzheimer’s research from a pool of passages describing potentially thousands Stanford\\nprofessors and Alzheimer’s researchers. Since current methods encode passages in isolation, they\\nwould struggle to identify Prof. Thomas unless a passage mentions both characteristics at once. In\\ncontrast, most people familiar with this professor would remember him quickly due to our brain’s\\nassociative memory capabilities, thought to be driven by the index structure depicted in the C-shaped\\nhippocampus above (in blue). Inspired by this mechanism, HippoRAG allows LLMs to build and\\nleverage a similar graph of associations to tackle knowledge integration tasks.\\nimportant real-world tasks, such as scientific literature review, legal case briefing, and medical\\ndiagnosis, require knowledge integration across passages or documents. Although less complex,\\nstandard multi-hop question answering (QA) also requires integrating information between passages\\nin a retrieval corpus. In order to solve such tasks, current RAG systems resort to using multiple\\nretrieval and LLM generation steps iteratively to join disparate passages [ 49,61]. Nevertheless, even\\nperfectly executed multi-step RAG is still oftentimes insufficient to accomplish many scenarios of\\nknowledge integration, as we illustrate in what we call path-finding multi-hop questions in Figure 1.\\nIn contrast, our brains are capable of solving challenging knowledge integration tasks like these with\\nrelative ease. The hippocampal memory indexing theory [ 58], a well-established theory of human\\nlong-term memory, offers one plausible explanation for this remarkable ability. Teyler and Discenna',\n",
       " 'named_entities': ['HippoRAG',\n",
       "  'Neurobiologically Inspired Long-Term Memory for Large Language Models',\n",
       "  'Bernal Jiménez Gutiérrez',\n",
       "  'The Ohio State University',\n",
       "  'Yiheng Shu',\n",
       "  'Yu Gu',\n",
       "  'Michihiro Yasunaga',\n",
       "  'Stanford University',\n",
       "  'Yu Su',\n",
       "  'mammalian brains',\n",
       "  'large language models (LLMs)',\n",
       "  'retrieval-augmented generation (RAG)',\n",
       "  'Personalized PageRank',\n",
       "  'neocortex',\n",
       "  'hippocampus',\n",
       "  'IRCoT',\n",
       "  'Alzheimer’s',\n",
       "  'Prof. Thomas',\n",
       "  'C-shaped hippocampus',\n",
       "  'scientific literature review',\n",
       "  'legal case briefing',\n",
       "  'medical diagnosis',\n",
       "  'multi-hop question answering (QA)',\n",
       "  'hippocampal memory indexing theory'],\n",
       " 'triples': [['HippoRAG',\n",
       "   'is inspired by',\n",
       "   'hippocampal memory indexing theory'],\n",
       "  ['HippoRAG', 'enables', 'knowledge integration'],\n",
       "  ['HippoRAG', 'is developed by', 'Bernal Jiménez Gutiérrez'],\n",
       "  ['HippoRAG', 'is developed by', 'Yiheng Shu'],\n",
       "  ['HippoRAG', 'is developed by', 'Yu Gu'],\n",
       "  ['HippoRAG', 'is developed by', 'Yu Su'],\n",
       "  ['HippoRAG', 'is developed by', 'Michihiro Yasunaga'],\n",
       "  ['Bernal Jiménez Gutiérrez', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Yiheng Shu', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Yu Gu', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Yu Su', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Michihiro Yasunaga', 'affiliated with', 'Stanford University'],\n",
       "  ['large language models (LLMs)', 'struggle with', 'knowledge integration'],\n",
       "  ['retrieval-augmented generation (RAG)',\n",
       "   'is a solution for',\n",
       "   'long-term memory in LLMs'],\n",
       "  ['HippoRAG', 'compares with', 'existing RAG methods'],\n",
       "  ['HippoRAG', 'outperforms', 'state-of-the-art methods'],\n",
       "  ['multi-hop question answering (QA)', 'requires', 'knowledge integration'],\n",
       "  ['C-shaped hippocampus', 'is related to', 'human memory'],\n",
       "  ['Prof. Thomas', 'works on', 'Alzheimer’s'],\n",
       "  ['mammalian brains', 'evolved to', 'store knowledge'],\n",
       "  ['mammalian brains', 'integrate', 'new experiences']]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks_with_entities_and_triples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:33:25 R2GWRJJGF9 root[40146] INFO Saving document structure to ../2405.14831v1_document_structure.pkl\n"
     ]
    }
   ],
   "source": [
    "document_structure_file = f\"{document_name.rsplit(\".\", 1)[0]}_document_structure.pkl\"\n",
    "save_document_structure(document_chunks_with_entities_and_triples, document_structure_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:33:31 R2GWRJJGF9 root[40146] INFO Initial Named Entities (24): ['hipporag', 'neurobiologically inspired long-term memory for large language models', 'bernal jiménez gutiérrez', 'the ohio state university', 'yiheng shu', 'yu gu', 'michihiro yasunaga', 'stanford university', 'yu su', 'mammalian brains', 'large language models (llms)', 'retrieval-augmented generation (rag)', 'personalized pagerank', 'neocortex', 'hippocampus', 'ircot', 'alzheimer’s', 'prof. thomas', 'c-shaped hippocampus', 'scientific literature review', 'legal case briefing', 'medical diagnosis', 'multi-hop question answering (qa)', 'hippocampal memory indexing theory']\n",
      "2024-11-05 11:33:31 R2GWRJJGF9 root[40146] INFO Initial Named Entities after dedup (24): {'stanford university', 'the ohio state university', 'yu su', 'hippocampal memory indexing theory', 'hippocampus', 'medical diagnosis', 'retrieval-augmented generation (rag)', 'ircot', 'large language models (llms)', 'neurobiologically inspired long-term memory for large language models', 'neocortex', 'scientific literature review', 'mammalian brains', 'hipporag', 'bernal jiménez gutiérrez', 'legal case briefing', 'yu gu', 'multi-hop question answering (qa)', 'personalized pagerank', 'yiheng shu', 'alzheimer’s', 'michihiro yasunaga', 'prof. thomas', 'c-shaped hippocampus'}\n",
      "2024-11-05 11:33:31 R2GWRJJGF9 root[40146] INFO Final Named Entities (31): {'stanford university', 'the ohio state university', 'yu su', 'hippocampal memory indexing theory', 'hippocampus', 'medical diagnosis', 'retrieval-augmented generation (rag)', 'ircot', 'large language models (llms)', 'neurobiologically inspired long-term memory for large language models', 'neocortex', 'scientific literature review', 'mammalian brains', 'hipporag', 'store knowledge', 'bernal jiménez gutiérrez', 'legal case briefing', 'yu gu', 'multi-hop question answering (qa)', 'personalized pagerank', 'human memory', 'state-of-the-art methods', 'new experiences', 'long-term memory in llms', 'yiheng shu', 'alzheimer’s', 'existing rag methods', 'michihiro yasunaga', 'prof. thomas', 'c-shaped hippocampus', 'knowledge integration'}\n"
     ]
    }
   ],
   "source": [
    "document_chunks_with_entities_and_triples = filter_named_entities(document_chunks_with_entities_and_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Document Structure after Filter Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:33:36 R2GWRJJGF9 root[40146] INFO Saving document structure to ../2405.14831v1_document_structure_with_ne.pkl\n"
     ]
    }
   ],
   "source": [
    "document_structure_file_with_ne = f\"{document_name.rsplit(\".\", 1)[0]}_document_structure_with_ne.pkl\"\n",
    "save_document_structure(document_chunks_with_entities_and_triples, document_structure_file_with_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': 'HippoRAG: Neurobiologically Inspired\\nLong-Term Memory for Large Language Models\\nBernal Jiménez Gutiérrez\\nThe Ohio State University\\njimenezgutierrez.1@osu.eduYiheng Shu\\nThe Ohio State University\\nshu.251@osu.edu\\nYu Gu\\nThe Ohio State University\\ngu.826@osu.eduMichihiro Yasunaga\\nStanford University\\nmyasu@cs.stanford.eduYu Su\\nThe Ohio State University\\nsu.809@osu.edu\\nAbstract\\nIn order to thrive in hostile and ever-changing natural environments, mammalian\\nbrains evolved to store large amounts of knowledge about the world and continually\\nintegrate new information while avoiding catastrophic forgetting. Despite the\\nimpressive accomplishments, large language models (LLMs), even with retrieval-\\naugmented generation (RAG), still struggle to efficiently and effectively integrate\\na large amount of new experiences after pre-training. In this work, we introduce\\nHippoRAG, a novel retrieval framework inspired by the hippocampal indexing\\ntheory of human long-term memory to enable deeper and more efficient knowledge\\nintegration over new experiences. HippoRAG synergistically orchestrates LLMs,\\nknowledge graphs, and the Personalized PageRank algorithm to mimic the different\\nroles of neocortex and hippocampus in human memory. We compare HippoRAG\\nwith existing RAG methods on multi-hop question answering and show that our\\nmethod outperforms the state-of-the-art methods remarkably, by up to 20%. Single-\\nstep retrieval with HippoRAG achieves comparable or better performance than\\niterative retrieval like IRCoT while being 10-30times cheaper and 6-13times faster,\\nand integrating HippoRAG into IRCoT brings further substantial gains. Finally,\\nwe show that our method can tackle new types of scenarios that are out of reach of\\nexisting methods.1\\n1 Introduction\\nMillions of years of evolution have led mammalian brains to develop the crucial ability to store large\\namounts of world knowledge and continuously integrate new experiences without losing previous\\nones. This exceptional long-term memory system eventually allows us humans to keep vast stores of\\ncontinuously updating knowledge that forms the basis of our reasoning and decision making [15].\\nDespite the progress of large language models (LLMs) in recent years, such a continuously updating\\nlong-term memory is still conspicuously absent from current AI systems. Due in part to its ease of\\nuse and the limitations of other techniques such as model editing [ 35], retrieval-augmented generation\\n(RAG) has become the de facto solution for long-term memory in LLMs, allowing users to present\\nnew knowledge to a static model [28, 33, 50].\\nHowever, current RAG methods are still unable to help LLMs perform tasks that require integrating\\nnew knowledge across passage boundaries since each new passage is encoded in isolation. Many\\n1Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG .\\nPreprint. Under review.arXiv:2405.14831v1  [cs.CL]  23 May 2024\\nHumanMemory Current RAG\\nHippoRAG Offline IndexingOnline Retrieval\\nAnswer:\\n!\\n!\\n!\\n!\\n\"\\n#\\n$\\n%\\n!\\n!Which     Stanford professor works on the neuroscience of     Alzheimer’s?\\n!\\n&\\n$\\n%\\n!\\n\"\\n\\'\\n&\\n#\\n&\\n%\\n!\\n\\'\\n#\\n\"\\nProf. ThomasFigure 1: Knowledge Integration & RAG. Tasks that require knowledge integration are particularly\\nchallenging for current RAG systems. In the above example, we want to find a Stanford professor\\nthat does Alzheimer’s research from a pool of passages describing potentially thousands Stanford\\nprofessors and Alzheimer’s researchers. Since current methods encode passages in isolation, they\\nwould struggle to identify Prof. Thomas unless a passage mentions both characteristics at once. In\\ncontrast, most people familiar with this professor would remember him quickly due to our brain’s\\nassociative memory capabilities, thought to be driven by the index structure depicted in the C-shaped\\nhippocampus above (in blue). Inspired by this mechanism, HippoRAG allows LLMs to build and\\nleverage a similar graph of associations to tackle knowledge integration tasks.\\nimportant real-world tasks, such as scientific literature review, legal case briefing, and medical\\ndiagnosis, require knowledge integration across passages or documents. Although less complex,\\nstandard multi-hop question answering (QA) also requires integrating information between passages\\nin a retrieval corpus. In order to solve such tasks, current RAG systems resort to using multiple\\nretrieval and LLM generation steps iteratively to join disparate passages [ 49,61]. Nevertheless, even\\nperfectly executed multi-step RAG is still oftentimes insufficient to accomplish many scenarios of\\nknowledge integration, as we illustrate in what we call path-finding multi-hop questions in Figure 1.\\nIn contrast, our brains are capable of solving challenging knowledge integration tasks like these with\\nrelative ease. The hippocampal memory indexing theory [ 58], a well-established theory of human\\nlong-term memory, offers one plausible explanation for this remarkable ability. Teyler and Discenna',\n",
       " 'named_entities': ['stanford university',\n",
       "  'the ohio state university',\n",
       "  'yu su',\n",
       "  'hippocampal memory indexing theory',\n",
       "  'hippocampus',\n",
       "  'medical diagnosis',\n",
       "  'retrieval-augmented generation (rag)',\n",
       "  'ircot',\n",
       "  'large language models (llms)',\n",
       "  'neurobiologically inspired long-term memory for large language models',\n",
       "  'neocortex',\n",
       "  'scientific literature review',\n",
       "  'mammalian brains',\n",
       "  'hipporag',\n",
       "  'store knowledge',\n",
       "  'bernal jiménez gutiérrez',\n",
       "  'legal case briefing',\n",
       "  'yu gu',\n",
       "  'multi-hop question answering (qa)',\n",
       "  'personalized pagerank',\n",
       "  'human memory',\n",
       "  'state-of-the-art methods',\n",
       "  'new experiences',\n",
       "  'long-term memory in llms',\n",
       "  'yiheng shu',\n",
       "  'alzheimer’s',\n",
       "  'existing rag methods',\n",
       "  'michihiro yasunaga',\n",
       "  'prof. thomas',\n",
       "  'c-shaped hippocampus',\n",
       "  'knowledge integration'],\n",
       " 'triples': [['HippoRAG',\n",
       "   'is inspired by',\n",
       "   'hippocampal memory indexing theory'],\n",
       "  ['HippoRAG', 'enables', 'knowledge integration'],\n",
       "  ['HippoRAG', 'is developed by', 'Bernal Jiménez Gutiérrez'],\n",
       "  ['HippoRAG', 'is developed by', 'Yiheng Shu'],\n",
       "  ['HippoRAG', 'is developed by', 'Yu Gu'],\n",
       "  ['HippoRAG', 'is developed by', 'Yu Su'],\n",
       "  ['HippoRAG', 'is developed by', 'Michihiro Yasunaga'],\n",
       "  ['Bernal Jiménez Gutiérrez', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Yiheng Shu', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Yu Gu', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Yu Su', 'affiliated with', 'The Ohio State University'],\n",
       "  ['Michihiro Yasunaga', 'affiliated with', 'Stanford University'],\n",
       "  ['large language models (LLMs)', 'struggle with', 'knowledge integration'],\n",
       "  ['retrieval-augmented generation (RAG)',\n",
       "   'is a solution for',\n",
       "   'long-term memory in LLMs'],\n",
       "  ['HippoRAG', 'compares with', 'existing RAG methods'],\n",
       "  ['HippoRAG', 'outperforms', 'state-of-the-art methods'],\n",
       "  ['multi-hop question answering (QA)', 'requires', 'knowledge integration'],\n",
       "  ['C-shaped hippocampus', 'is related to', 'human memory'],\n",
       "  ['Prof. Thomas', 'works on', 'Alzheimer’s'],\n",
       "  ['mammalian brains', 'evolved to', 'store knowledge'],\n",
       "  ['mammalian brains', 'integrate', 'new experiences']]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks_with_entities_and_triples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dedup Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2uid_dict = create_document_deduped_entities_dict(document_chunks_with_entities_and_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stanford university': 0,\n",
       " 'the ohio state university': 1,\n",
       " 'yu su': 2,\n",
       " 'hippocampal memory indexing theory': 3,\n",
       " 'hippocampus': 4,\n",
       " 'medical diagnosis': 5,\n",
       " 'retrieval-augmented generation (rag)': 6,\n",
       " 'ircot': 7,\n",
       " 'large language models (llms)': 8,\n",
       " 'neurobiologically inspired long-term memory for large language models': 9,\n",
       " 'neocortex': 10,\n",
       " 'scientific literature review': 11,\n",
       " 'mammalian brains': 12,\n",
       " 'hipporag': 13,\n",
       " 'store knowledge': 14,\n",
       " 'bernal jiménez gutiérrez': 15,\n",
       " 'legal case briefing': 16,\n",
       " 'yu gu': 17,\n",
       " 'multi-hop question answering (qa)': 18,\n",
       " 'personalized pagerank': 19,\n",
       " 'human memory': 20,\n",
       " 'state-of-the-art methods': 21,\n",
       " 'new experiences': 22,\n",
       " 'long-term memory in llms': 23,\n",
       " 'yiheng shu': 24,\n",
       " 'alzheimer’s': 25,\n",
       " 'existing rag methods': 26,\n",
       " 'michihiro yasunaga': 27,\n",
       " 'prof. thomas': 28,\n",
       " 'c-shaped hippocampus': 29,\n",
       " 'knowledge integration': 30}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity2uid_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the entity to uid dict to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:38:08 R2GWRJJGF9 root[40146] INFO entity2uid_dict has been saved to ../2405.14831v1_entity2uid_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{document_name.rsplit(\".\", 1)[0]}_entity2uid_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity2uid_dict, f)\n",
    "logger.info(f\"entity2uid_dict has been saved to {document_name.rsplit(\".\", 1)[0]}_entity2uid_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>.<span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m9\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = create_matrix_entity_ref_count(document_chunks_with_entities_and_triples, entity2uid_dict)\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "print(matrix)\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 0 stanford university Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 1 the ohio state university Per chunk count: [4.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 2 yu su Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 3 hippocampal memory indexing theory Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 4 hippocampus Per chunk count: [2.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 5 medical diagnosis Per chunk count: [0.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 6 retrieval-augmented generation (rag) Per chunk count: [0.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 7 ircot Per chunk count: [2.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 8 large language models (llms) Per chunk count: [2.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 9 neurobiologically inspired long-term memory for large language models Per chunk count: [0.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 10 neocortex Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 11 scientific literature review Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 12 mammalian brains Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 13 hipporag Per chunk count: [9.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 14 store knowledge Per chunk count: [0.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 15 bernal jiménez gutiérrez Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 16 legal case briefing Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 17 yu gu Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 18 multi-hop question answering (qa) Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 19 personalized pagerank Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 20 human memory Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 21 state-of-the-art methods Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 22 new experiences Per chunk count: [3.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 23 long-term memory in llms Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 24 yiheng shu Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 25 alzheimer’s Per chunk count: [3.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 26 existing rag methods Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 27 michihiro yasunaga Per chunk count: [1.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 28 prof. thomas Per chunk count: [2.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 29 c-shaped hippocampus Per chunk count: [0.]\n",
      "2024-11-05 11:40:20 R2GWRJJGF9 root[40146] INFO Entity: 30 knowledge integration Per chunk count: [6.]\n"
     ]
    }
   ],
   "source": [
    "example_chunk = document_chunks_with_entities_and_triples[0]\n",
    "\n",
    "n_of_entities = len(entity2uid_dict)\n",
    "n_of_chunks = len(document_chunks_with_entities_and_triples)\n",
    "\n",
    "for e_idx in range(n_of_entities):\n",
    "    entity_name = list(entity2uid_dict.keys())[list(entity2uid_dict.values()).index(e_idx)]\n",
    "    logger.info(f\"Entity: {e_idx} {entity_name} Per chunk count: {matrix[e_idx][:]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the matrix to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:40:23 R2GWRJJGF9 root[40146] INFO Entity per chunk count matrix has been saved to ../2405.14831v1_entity_per_chunk_count_matrix.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{document_name.rsplit(\".\", 1)[0]}_entity_per_chunk_count_matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(matrix, f)\n",
    "logger.info(f\"Entity per chunk count matrix has been saved to {document_name.rsplit(\".\", 1)[0]}_entity_per_chunk_count_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings and FAISS index params\n",
    "emb_dimension = 256\n",
    "recall_at_k = 3  # how far in the indices/distances we go\n",
    "\n",
    "# M_max defines the maximum number of links a vertex can have, and M_max0, which defines the same but for vertices in layer 0.\n",
    "M = 64  # for HNSW index, the number of neighbors we add to each vertex on insertion. \n",
    "# Faiss sets M_max and M_max0 automatically in the set_default_probas method, at index initialization. \n",
    "# The M_max value is set to M, and M_max0 set to M*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings for named entities in document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:12:26 R2GWRJJGF9 root[36374] INFO Named entities dict loaded: {'stanford university': 0, 'the ohio state university': 1, 'yu su': 2, 'hippocampal memory indexing theory': 3, 'hippocampus': 4, 'medical diagnosis': 5, 'retrieval-augmented generation (rag)': 6, 'ircot': 7, 'large language models (llms)': 8, 'neurobiologically inspired long-term memory for large language models': 9, 'neocortex': 10, 'scientific literature review': 11, 'mammalian brains': 12, 'hipporag': 13, 'store knowledge': 14, 'bernal jiménez gutiérrez': 15, 'legal case briefing': 16, 'yu gu': 17, 'multi-hop question answering (qa)': 18, 'personalized pagerank': 19, 'human memory': 20, 'state-of-the-art methods': 21, 'new experiences': 22, 'long-term memory in llms': 23, 'yiheng shu': 24, 'alzheimer’s': 25, 'existing rag methods': 26, 'michihiro yasunaga': 27, 'prof. thomas': 28, 'c-shaped hippocampus': 29, 'knowledge integration': 30}\n",
      "2024-11-05 13:12:26 R2GWRJJGF9 root[36374] INFO Number of entities: 31. First entity is: stanford university\n"
     ]
    }
   ],
   "source": [
    "named_entities_dict = pickle.loads(open(f\"{document_name.rsplit(\".\", 1)[0]}_entity2uid_dict.pkl\", \"rb\").read())\n",
    "logger.info(f\"Named entities dict loaded: {named_entities_dict}\")\n",
    "entities = list(named_entities_dict.keys())\n",
    "logger.info(f\"Number of entities: {len(entities)}. First entity is: {entities[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate (and save) entity embeddings and convert them to np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filepath = f\"{document_name.rsplit('.', 1)[0]}_entity_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entities_embeddings = generate_entity_embeddings(entities, emb_dimension, embeddings_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Step: Load the entity Embeddings (Just to continue from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_embeddings = pickle.loads(open(embeddings_filepath, \"rb\").read())\n",
    "entities_embeddings = np.array(entities_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index and Query It with the same elements we indexed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:14:53 R2GWRJJGF9 root[36374] INFO \n",
      "Distances:\n",
      "[[0.    0.985 1.282]\n",
      " [0.    0.985 1.469]\n",
      " [0.    0.799 0.932]\n",
      " [0.    0.858 1.041]\n",
      " [0.    0.763 0.858]\n",
      " [0.    1.318 1.383]\n",
      " [0.    1.022 1.259]\n",
      " [0.    1.392 1.396]\n",
      " [0.    0.748 0.76 ]\n",
      " [0.    0.699 0.76 ]\n",
      " [0.    1.014 1.136]\n",
      " [0.    1.276 1.406]\n",
      " [0.    1.161 1.164]\n",
      " [0.    0.882 1.227]\n",
      " [0.    0.853 1.268]\n",
      " [0.    1.298 1.483]\n",
      " [0.    1.428 1.521]\n",
      " [0.    0.799 0.927]\n",
      " [0.    1.313 1.33 ]\n",
      " [0.    1.345 1.391]\n",
      " [0.    1.046 1.05 ]\n",
      " [0.    1.276 1.278]\n",
      " [0.    1.278 1.281]\n",
      " [0.    0.699 0.748]\n",
      " [0.    0.927 0.932]\n",
      " [0.    1.158 1.268]\n",
      " [0.    1.022 1.279]\n",
      " [0.    1.063 1.274]\n",
      " [0.    1.282 1.298]\n",
      " [0.    0.763 1.066]\n",
      " [0.    0.853 1.26 ]]\n",
      "Indices:\n",
      "[[ 0  1 28]\n",
      " [ 1  0 28]\n",
      " [ 2 17 24]\n",
      " [ 3  4 23]\n",
      " [ 4 29  3]\n",
      " [ 5 25  2]\n",
      " [ 6 26 13]\n",
      " [ 7 19  6]\n",
      " [ 8 23  9]\n",
      " [ 9 23  8]\n",
      " [10  4 29]\n",
      " [11 21  5]\n",
      " [12 23  4]\n",
      " [13  4 29]\n",
      " [14 30  3]\n",
      " [15 28  2]\n",
      " [16 11 28]\n",
      " [17  2 24]\n",
      " [18  8  6]\n",
      " [19  6  3]\n",
      " [20  4  3]\n",
      " [21 11 22]\n",
      " [22 21 14]\n",
      " [23  9  8]\n",
      " [24 17  2]\n",
      " [25  4 20]\n",
      " [26  6 21]\n",
      " [27  2 17]\n",
      " [28  0 15]\n",
      " [29  4  3]\n",
      " [30 14  3]]\n"
     ]
    }
   ],
   "source": [
    "faiss_index = create_index(entities_embeddings, emb_dimension, M)\n",
    "distances, indexes = search_index(faiss_index, entities_embeddings, recall_at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Similar Entities with Recall at K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for yu su (2) with yu gu (17): Distance 0.7987977266311646\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for hippocampus (4) with c-shaped hippocampus (29): Distance 0.7634723782539368\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for large language models (llms) (8) with long-term memory in llms (23): Distance 0.7482843399047852\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for large language models (llms) (8) with neurobiologically inspired long-term memory for large language models (9): Distance 0.7598475217819214\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for neurobiologically inspired long-term memory for large language models (9) with long-term memory in llms (23): Distance 0.6987998485565186\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for neurobiologically inspired long-term memory for large language models (9) with large language models (llms) (8): Distance 0.7598475217819214\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for yu gu (17) with yu su (2): Distance 0.7987977266311646\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for long-term memory in llms (23) with neurobiologically inspired long-term memory for large language models (9): Distance 0.6987998485565186\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for long-term memory in llms (23) with large language models (llms) (8): Distance 0.7482843399047852\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similarity (<=0.85 dist) found for c-shaped hippocampus (29) with hippocampus (4): Distance 0.7634723782539368\n",
      "2024-11-05 13:15:04 R2GWRJJGF9 root[36374] INFO Similar entities:\n",
      "[{'entity': 'yu su', 'similar_entity': 'yu gu'}, {'entity': 'hippocampus', 'similar_entity': 'c-shaped hippocampus'}, {'entity': 'large language models (llms)', 'similar_entity': 'long-term memory in llms'}, {'entity': 'large language models (llms)', 'similar_entity': 'neurobiologically inspired long-term memory for large language models'}, {'entity': 'neurobiologically inspired long-term memory for large language models', 'similar_entity': 'long-term memory in llms'}, {'entity': 'neurobiologically inspired long-term memory for large language models', 'similar_entity': 'large language models (llms)'}, {'entity': 'yu gu', 'similar_entity': 'yu su'}, {'entity': 'long-term memory in llms', 'similar_entity': 'neurobiologically inspired long-term memory for large language models'}, {'entity': 'long-term memory in llms', 'similar_entity': 'large language models (llms)'}, {'entity': 'c-shaped hippocampus', 'similar_entity': 'hippocampus'}]\n"
     ]
    }
   ],
   "source": [
    "similar_entities = build_similar_entities(entities, indexes, distances, recall_at_k, max_distance=0.85)  # Original max_distance=0.7\n",
    "logger.info(f\"Similar entities:\\n{similar_entities}\")\n",
    "\n",
    "# TODO Scores discarded for now\n",
    "# scores = calculate_scores(distances)\n",
    "# similar_entities_score = build_similar_entities_with_scores(entities, indexes, scores, recall_at_k, min_score=0.5)            \n",
    "# logger.info(similar_entities_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neo4J Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = os.environ[\"NEO4J_URI\"]\n",
    "NEO4J_USERNAME = os.environ[\"NEO4J_USERNAME\"]\n",
    "NEO4J_PASSWORD = os.environ[\"NEO4J_PASSWORD\"]\n",
    "NEO4J_DATABASE = os.environ[\"NEO4J_DB\"] \n",
    "\n",
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required to create the graph:\n",
    "    - entity_embeddings\n",
    "    - named entities dict \n",
    "    - doc structure\n",
    "    - similar entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Add all entities to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:10:28 R2GWRJJGF9 root[36374] INFO Cleaning Neo4j database\n",
      "2024-11-05 13:10:29 R2GWRJJGF9 root[36374] INFO Result after deleting nodes:\n",
      "[]\n",
      "2024-11-05 13:10:29 R2GWRJJGF9 root[36374] INFO Result after properties:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "clean_db(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entities(kg, entities_embeddings, named_entities_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add RELATES_TO relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:13:02 R2GWRJJGF9 neo4j.notifications[36374] INFO Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (b))} {position: line: 3, column: 5, offset: 37} for query: '\\n    UNWIND $triplets AS triplet\\n    MATCH (a:Entity {name: triplet.subject}), (b:Entity {name: triplet.object})\\n    MERGE (a)-[:RELATES_TO {type: triplet.predicate}]->(b)\\n    '\n"
     ]
    }
   ],
   "source": [
    "doc_structure = pickle.loads(open(f\"{document_name.rsplit(\".\", 1)[0]}_document_structure_with_ne.pkl\", \"rb\").read())\n",
    "add_relates_to_relationships(kg, doc_structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Add SIMILAR_TO relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:15:08 R2GWRJJGF9 neo4j.notifications[36374] INFO Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (b))} {position: line: 3, column: 5, offset: 40} for query: '\\n    UNWIND $similar_entities AS se\\n    MATCH (a:Entity {name: se.entity}), (b:Entity {name: se.similar_entity})\\n    MERGE (a)-[:SIMILAR_TO]->(b)\\n    '\n"
     ]
    }
   ],
   "source": [
    "add_similar_entities(kg, similar_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:23:47 R2GWRJJGF9 neo4j.notifications[36374] INFO Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE VECTOR INDEX entityIdx IF NOT EXISTS FOR (e:Entity) ON (e.embedding) OPTIONS {indexConfig: {`vector.dimensions`: $emb_dim, `vector.similarity_function`: $sim_func}}` has no effect.} {description: `VECTOR INDEX entityIdx FOR (e:Entity) ON (e.embedding)` already exists.} {position: None} for query: '\\n    CREATE VECTOR INDEX $idx_name IF NOT EXISTS\\n    FOR (m:Entity)\\n    ON m.embedding\\n    OPTIONS {indexConfig: {\\n        `vector.dimensions`: $emb_dim,\\n        `vector.similarity_function`: $sim_func\\n    }}\\n    '\n"
     ]
    }
   ],
   "source": [
    "build_vector_index(kg, emb_dim=emb_dimension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
